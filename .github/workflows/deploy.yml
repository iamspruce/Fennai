name: Deploy to Firebase & Cloud Run

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]

jobs:
  deploy-hosting:
    name: Deploy Frontend (Firebase Hosting)
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install dependencies
        run: npm ci

      - name: Create .env file
        run: |
          echo "${{ secrets.ENV_FILE }}" > .env
          echo ".env created."

      - name: Deploy to Firebase Hosting
        uses: FirebaseExtended/action-hosting-deploy@v0
        with:
          repoToken: ${{ secrets.GITHUB_TOKEN }}
          firebaseServiceAccount: ${{ secrets.FIREBASE_SERVICE_ACCOUNT }}
          projectId: fennai
          channelId: live
        env:
          FIREBASE_CLI_EXPERIMENTS: webframeworks

  deploy-backend:
    name: Deploy Functions + Cloud Run GPU Inference
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v')  # Only on version tags
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Node
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.FIREBASE_SERVICE_ACCOUNT }}

      - name: Set up Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: fennai

      - name: Install Firebase CLI
        run: npm install -g firebase-tools

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Setup Python virtual environment for proxy function
        run: |
          cd functions/proxy
          python3.10 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # CREATE CLOUD STORAGE BUCKET (idempotent)
      - name: Create Cloud Storage Bucket for Models (if not exists)
        run: |
          BUCKET_NAME="fennai-vibevoice-models"
          REGION="us-central1"
          
          if gsutil ls -b gs://$BUCKET_NAME 2>/dev/null; then
            echo "‚úì Bucket gs://$BUCKET_NAME already exists"
          else
            echo "Creating bucket gs://$BUCKET_NAME"
            gsutil mb -p fennai -c STANDARD -l $REGION gs://$BUCKET_NAME
            echo "‚úì Bucket created successfully"
          fi

      # NEW: Pre-download model to bucket if not present
      - name: Pre-download Model to Bucket (One-Time)
        run: |
          BUCKET_NAME="fennai-vibevoice-models"
          MODEL_NAME="microsoft/VibeVoice-1.5B"
          BUCKET_PATH="gs://$BUCKET_NAME/VibeVoice-1.5B"
          
          echo "Checking if model exists in bucket..."
          
          # Check for HuggingFace's own metadata cache
          if gsutil -q stat "$BUCKET_PATH/.cache/huggingface/download/.locks/models--microsoft--VibeVoice-1.5B/.gitignore" 2>/dev/null; then
            echo "‚úì HuggingFace metadata found - model was previously downloaded successfully"
            
            # Quick sanity check
            if gsutil -q stat "$BUCKET_PATH/config.json" 2>/dev/null; then
              echo "‚úì Model verified. Skipping download."
              exit 0
            else
              echo "‚ö†Ô∏è Metadata exists but model files missing. Re-downloading..."
            fi
          fi
          
          echo "‚¨áÔ∏è Downloading model to bucket..."
          echo "This is a one-time operation (~3GB, may take 2-5 minutes)"
          
          # Install huggingface-hub
          pip install huggingface-hub
          
          # Download model using HuggingFace's built-in caching/resume
          python3 << 'PYTHON_SCRIPT'
          import os
          import sys
          from pathlib import Path
          from huggingface_hub import snapshot_download
          
          print("Downloading model from HuggingFace...")
          try:
              # Download to temp location - HF handles completion tracking
              snapshot_dir = snapshot_download(
                  repo_id="microsoft/VibeVoice-1.5B",
                  local_dir="./temp_model",
                  local_dir_use_symlinks=False,
                  resume_download=True,  # HF handles partial downloads
                  token=os.getenv("HF_TOKEN"),
                  ignore_patterns=["*.msgpack", "*.h5"],
              )
              print(f"‚úì HuggingFace completed download to {snapshot_dir}")
              
          except Exception as e:
              print(f"‚ùå Download failed: {e}")
              sys.exit(1)
          PYTHON_SCRIPT
          
          if [ $? -ne 0 ]; then
            echo "‚ùå Model download failed"
            exit 1
          fi
          
          # Upload entire directory including HF's cache metadata
          echo "Uploading to bucket (including HuggingFace metadata)..."
          gsutil -m rsync -r ./temp_model "$BUCKET_PATH"
          
          if [ $? -ne 0 ]; then
            echo "‚ùå Upload to bucket failed"
            rm -rf ./temp_model
            exit 1
          fi
          
          # Cleanup
          rm -rf ./temp_model
          
          echo "‚úì Model successfully uploaded to bucket"
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}

      - name: Verify Model in Bucket (Final Check)
        run: |
          BUCKET_PATH="gs://fennai-vibevoice-models/VibeVoice-1.5B"
          
          echo "Running final model verification..."
          
          # Check critical model files exist
          REQUIRED_FILES=(
            "config.json"
            "preprocessor_config.json"
            "generation_config.json"
          )
          
          MISSING=""
          for file in "${REQUIRED_FILES[@]}"; do
            if ! gsutil -q stat "$BUCKET_PATH/$file" 2>/dev/null; then
              MISSING="$MISSING $file"
            fi
          done
          
          if [ -n "$MISSING" ]; then
            echo "‚ùå Verification failed. Missing files:$MISSING"
            exit 1
          fi
          
          echo "‚úì All critical files present"
          echo "‚úì Model ready for deployment"

      - name: Build & Submit GPU Inference Image
        run: |
          gcloud builds submit \
            --tag gcr.io/fennai/fennai-inference:latest \
            --timeout=3600s \
            functions/inference

      - name: Deploy Inference to Cloud Run (with Cloud Storage Mount)
        id: deploy_run
        run: |
          URL=$(gcloud run deploy fennai-inference \
            --image gcr.io/fennai/fennai-inference:latest \
            --platform managed \
            --region us-central1 \
            --execution-environment gen2 \
            --allow-unauthenticated \
            --cpu 8 \
            --memory 32Gi \
            --gpu 1 \
            --gpu-type nvidia-l4 \
            --no-gpu-zonal-redundancy \
            --max-instances 1 \
            --min-instances 0 \
            --timeout 900s \
            --port 8080 \
            --cpu-boost \
            --concurrency=1 \
            --set-env-vars INTERNAL_TOKEN=${{ secrets.INTERNAL_TOKEN }},MODEL_NAME=microsoft/VibeVoice-1.5B,HF_TOKEN=${{ secrets.HF_TOKEN }},CACHE_DIR=/models \
            --add-volume name=model-volume,type=cloud-storage,bucket=fennai-vibevoice-models \
            --add-volume-mount volume=model-volume,mount-path=/models \
            --quiet \
            --format='value(status.url)')
          echo "CLOUD_RUN_URL=$URL" >> $GITHUB_OUTPUT
          echo "CLOUD_RUN_URL=$URL" >> $GITHUB_ENV

      - name: Verify Deployment
        run: |
          echo "Waiting for Cloud Run to be ready..."
          sleep 10
          
          HEALTH_URL="${{ steps.deploy_run.outputs.CLOUD_RUN_URL }}/health"
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$HEALTH_URL" || echo "000")
          
          if [ "$RESPONSE" = "200" ] || [ "$RESPONSE" = "503" ]; then
            echo "‚úì Cloud Run is responding (status: $RESPONSE)"
            echo "Note: 503 is normal if model needs to load on first request"
          else
            echo "‚ö† Unexpected response: $RESPONSE"
            echo "Cloud Run may still be initializing"
          fi

      - name: Create .env for proxy function
        run: |
          cd functions/proxy
          echo "INFERENCE_URL=${{ steps.deploy_run.outputs.CLOUD_RUN_URL }}" > .env
          echo "INTERNAL_TOKEN=${{ secrets.INTERNAL_TOKEN }}" >> .env

      - name: Deploy Proxy Function
        run: |
          firebase deploy \
            --only functions:proxy \
            --project fennai \
            --non-interactive

      - name: Deployment Summary
        run: |
          echo "================================"
          echo "üöÄ DEPLOYMENT SUCCESSFUL"
          echo "================================"
          echo "Cloud Run URL: ${{ steps.deploy_run.outputs.CLOUD_RUN_URL }}"
          echo ""
          echo "Storage Configuration:"
          echo "  ‚Ä¢ Type: Cloud Storage Bucket (GCS FUSE)"
          echo "  ‚Ä¢ Bucket: gs://fennai-vibevoice-models"
          echo "  ‚Ä¢ Mount Path: /models"
          echo "  ‚Ä¢ Model: microsoft/VibeVoice-1.5B (~3GB)"
          echo "  ‚Ä¢ Status: Pre-downloaded and cached"
          echo ""
          echo "Performance:"
          echo "  ‚Ä¢ First container start: 5-10s (model read from bucket)"
          echo "  ‚Ä¢ Inference requests: <2s"
          echo "  ‚Ä¢ Cold starts: 5-10s (model already in bucket)"
          echo ""
          echo "================================"